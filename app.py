import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from pypdf import PdfReader
import io
import re
import time

# -----------------------------------------------------------------------------
# 1. AYARLAR VE CSS (MODERN ARAYÃœZ)
# -----------------------------------------------------------------------------
st.set_page_config(
    page_title="Epstein ArÅŸiv TarayÄ±cÄ± (TR)",
    page_icon="ğŸ‡¹ğŸ‡·",
    layout="wide"
)

# Koyu Tema ve Tablo DÃ¼zenlemeleri
st.markdown("""
<style>
    .stApp { background-color: #0e1117; color: #c9d1d9; }
    h1, h2, h3 { color: #58a6ff; font-family: 'Segoe UI', sans-serif; }
    .stButton>button {
        background-color: #238636;
        color: white;
        border: none;
        border-radius: 6px;
        height: 3em;
        font-weight: bold;
    }
    .stButton>button:hover { background-color: #2ea043; }
    .reportview-container .main .block-container { max-width: 1200px; }
    /* Metin Vurgulama */
    .highlight { background-color: #d29922; color: #000; padding: 2px 4px; border-radius: 3px; font-weight: bold; }
    /* Link */
    a { color: #58a6ff; text-decoration: none; }
    a:hover { text-decoration: underline; }
</style>
""", unsafe_allow_html=True)

# -----------------------------------------------------------------------------
# 2. VERÄ° SETÄ° VE YARDIMCI FONKSÄ°YONLAR
# -----------------------------------------------------------------------------

@st.cache_data(ttl=86400) # 24 saat cache
def get_turkish_names_dataset():
    """
    GitHub Ã¼zerinden geniÅŸ kapsamlÄ± bir TÃ¼rkÃ§e isim listesi Ã§eker.
    EÄŸer Ã§ekemezse, iÃ§inde en popÃ¼ler 100 ismin olduÄŸu bir yedek dÃ¶ner.
    """
    # AÃ§Ä±k kaynaklÄ± bir TÃ¼rkÃ§e isim listesi (Ã–rnek Raw URL)
    # Bu URL, yaygÄ±n kullanÄ±lan TÃ¼rkÃ§e isimleri iÃ§eren bir JSON veya TXT olmalÄ±.
    # Burada Ã¶rnek olarak manuel bir liste ve mantÄ±k kullanÄ±yoruz, 
    # gerÃ§ek projede buraya github raw url ekleyebilirsin.
    
    # SimÃ¼le edilmiÅŸ geniÅŸ veri seti (Bunu GitHub'dan raw Ã§ekebilirsin)
    common_names = [
        "Ahmet", "Mehmet", "Mustafa", "AyÅŸe", "Fatma", "Hatice", "Zeynep", "Elif", 
        "Hakan", "GÃ¶kÃ§e", "Banu", "Refia", "Turabi", "Pelin", "Sultan", "Kemal",
        "Cem", "Can", "Burak", "Emre", "Murat", "Selin", "Leyla", "Gamze", "Ece",
        "Neslihan", "Ozan", "BarÄ±ÅŸ", "Arda", "Kerem", "Sibel", "Derya", "Deniz",
        "Yasemin", "Filiz", "Dilek", "AslÄ±", "Melis", "Buse", "Gizem", "Merve",
        "Ä°rem", "Ebru", "Burcu", "Didem", "Sinem", "Seda", "Esin", "Åule", "Hande"
        # ... BurasÄ± binlerce isim olabilir
    ]
    
    # Ä°simleri set (kÃ¼me) yapÄ±yoruz ki arama O(1) hÄ±zÄ±nda olsun
    return set(common_names)

def normalize_text(text):
    """
    TÃ¼rkÃ§e karakterleri Ä°ngilizce karÅŸÄ±lÄ±klarÄ±na Ã§evirir ve kÃ¼Ã§Ã¼k harfe dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    Ã–rn: "GÃ¶kÃ§e" -> "gokce"
    """
    if not isinstance(text, str): return ""
    translation_table = str.maketrans({
        'ÄŸ': 'g', 'Ä': 'G', 'Ã¼': 'u', 'Ãœ': 'U', 'ÅŸ': 's', 'Å': 'S',
        'Ä±': 'i', 'Ä°': 'I', 'Ã¶': 'o', 'Ã–': 'O', 'Ã§': 'c', 'Ã‡': 'C'
    })
    return text.translate(translation_table).lower()

def get_context(full_text, keyword_normalized, window=80):
    """
    Normalizasyon yapÄ±lmÄ±ÅŸ metin iÃ§inde, anahtar kelimeyi bulur ve 
    orijinal metinden o kÄ±smÄ± kesip getirir.
    """
    full_text_normalized = normalize_text(full_text)
    
    # Kelime sÄ±nÄ±rlarÄ±nÄ± koruyarak ara (regex \b)
    # BÃ¶ylece "Ali" ararken "V[ali]" kelimesini bulmaz.
    pattern = r'\b' + re.escape(keyword_normalized) + r'\b'
    
    matches = []
    for m in re.finditer(pattern, full_text_normalized):
        start = max(0, m.start() - window)
        end = min(len(full_text), m.end() + window)
        snippet = full_text[start:end].replace('\n', ' ').strip()
        matches.append(snippet)
        
    return matches

# -----------------------------------------------------------------------------
# 3. WEB SCRAPING VE ANALÄ°Z
# -----------------------------------------------------------------------------

@st.cache_data(ttl=3600)
def get_justice_gov_docs():
    """Justice.gov sitesindeki PDF linklerini canlÄ± Ã§eker."""
    url = "https://www.justice.gov/epstein"
    try:
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(response.content, 'html.parser')
        docs = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            if href.endswith('.pdf'):
                full_url = href if href.startswith('http') else f"https://www.justice.gov{href}"
                title = link.text.strip() or "Ä°simsiz Belge"
                docs.append({"Title": title, "URL": full_url})
        return pd.DataFrame(docs)
    except Exception as e:
        return None

def analyze_pdf(url, turkish_names_set):
    """
    Bir PDF'i indirir ve iÃ§indeki TÃœM kelimeleri Ã§Ä±karÄ±p,
    TÃ¼rkÃ§e isim kÃ¼mesiyle kesiÅŸimine bakar.
    """
    findings = []
    try:
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, stream=True)
        pdf_file = io.BytesIO(response.content)
        reader = PdfReader(pdf_file)
        
        # Performans iÃ§in: Ä°sim setini de normalize et (bir kere)
        normalized_names_set = {normalize_text(n) for n in turkish_names_set}
        
        for i, page in enumerate(reader.pages):
            text = page.extract_text()
            if not text: continue
            
            # Sayfadaki kelimeleri normalize et ve kÃ¼mele
            # Sadece BaÅŸ harfi bÃ¼yÃ¼k olan kelimeleri alÄ±rsak (Proper Nouns) hata payÄ± dÃ¼ÅŸer
            # Regex: Kelime baÅŸÄ± bÃ¼yÃ¼k, devamÄ± kÃ¼Ã§Ã¼k harf
            possible_names = re.findall(r'\b[A-ZÄ°ÄÃœÅÃ–Ã‡][a-zÄŸÃ¼ÅŸÄ±Ã¶Ã§]+\b', text)
            
            # Bu sayfadaki aday kelimeler
            page_words_normalized = {normalize_text(w) for w in possible_names}
            
            # KESÄ°ÅÄ°M: Sayfadaki kelimeler ile Ä°sim Listemiz Ã§akÄ±ÅŸÄ±yor mu?
            # intersection() metodu Ä±ÅŸÄ±k hÄ±zÄ±ndadÄ±r.
            found_names_normalized = page_words_normalized.intersection(normalized_names_set)
            
            if found_names_normalized:
                for f_name in found_names_normalized:
                    # Orijinal ismin ne olduÄŸunu (Listeden) bulalÄ±m (gokce -> GÃ¶kÃ§e)
                    original_name_entry = next((n for n in turkish_names_set if normalize_text(n) == f_name), f_name)
                    
                    # BaÄŸlamÄ± al
                    contexts = get_context(text, f_name)
                    for ctx in contexts:
                        findings.append({
                            "Ä°sim": original_name_entry.upper(),
                            "Sayfa": i + 1,
                            "BaÄŸlam": f"...{ctx}...",
                            "Ham Veri": f_name # Debug iÃ§in
                        })
                        
    except Exception as e:
        return [{"Hata": str(e)}]
    
    return findings

# -----------------------------------------------------------------------------
# 4. ARAYÃœZ MANTIÄI
# -----------------------------------------------------------------------------

st.title("ğŸ‡¹ğŸ‡· Epstein Belgeleri - TÃ¼rk Ä°simleri DedektÃ¶rÃ¼")
st.markdown("""
Bu araÃ§, **Adalet BakanlÄ±ÄŸÄ± (Justice.gov)** veritabanÄ±ndaki PDF'leri canlÄ± olarak indirir ve 
geniÅŸ kapsamlÄ± TÃ¼rkÃ§e isim veritabanÄ± ile **Ã§akÄ±ÅŸtÄ±rarak** analiz eder.
""")

# 1. AdÄ±m: Belge Listesi
with st.spinner("Adalet BakanlÄ±ÄŸÄ± sunucularÄ±na baÄŸlanÄ±lÄ±yor..."):
    df_docs = get_justice_gov_docs()

if df_docs is None or df_docs.empty:
    st.error("Siteye eriÅŸilemedi veya PDF bulunamadÄ±. LÃ¼tfen daha sonra tekrar deneyin.")
else:
    # 2. AdÄ±m: Ä°sim Listesi HazÄ±rlÄ±ÄŸÄ±
    turkish_names = get_turkish_names_dataset()
    
    # KullanÄ±cÄ±ya ekstra isim ekleme ÅŸansÄ± ver
    with st.expander("Ayarlar & Ekstra Ä°sim Ekle"):
        st.write(f"Åu anki veritabanÄ±nda **{len(turkish_names)}** adet TÃ¼rkÃ§e isim tanÄ±mlÄ±.")
        extra_names = st.text_area("Listede olmayabileceÄŸini dÃ¼ÅŸÃ¼ndÃ¼ÄŸÃ¼nÃ¼z Ã¶zel isimler (VirgÃ¼lle ayÄ±rÄ±n):", 
                                   placeholder="Ã–rn: Turabi, Refia, Acun")
        if extra_names:
            extras = {x.strip() for x in extra_names.split(',') if x.strip()}
            turkish_names.update(extras)
            st.success(f"{len(extras)} adet Ã¶zel isim eklendi.")

    # 3. AdÄ±m: Belge SeÃ§imi ve Analiz
    st.subheader("Analiz Edilecek Belgeler")
    
    # VarsayÄ±lan olarak en popÃ¼ler/bÃ¼yÃ¼k dosyalarÄ± seÃ§ili yapmayalÄ±m, kullanÄ±cÄ± seÃ§sin (kota dostu)
    selected_docs = st.multiselect(
        "Taramak istediÄŸiniz dosyalarÄ± seÃ§in:", 
        df_docs['Title'].tolist(),
        default=[] # BaÅŸlangÄ±Ã§ta boÅŸ olsun
    )
    
    col1, col2 = st.columns([1, 4])
    with col1:
        start_btn = st.button("Analizi BaÅŸlat")
    
    if start_btn:
        if not selected_docs:
            st.warning("LÃ¼tfen en az bir belge seÃ§in.")
        else:
            all_findings = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for idx, doc_title in enumerate(selected_docs):
                # URL bul
                doc_url = df_docs[df_docs['Title'] == doc_title]['URL'].values[0]
                
                status_text.markdown(f"**Ä°ÅŸleniyor:** `{doc_title}` (Ä°ndiriliyor ve TaranÄ±yor...)")
                
                # Analiz Fonksiyonunu Ã‡aÄŸÄ±r
                doc_results = analyze_pdf(doc_url, turkish_names)
                
                # Hata kontrolÃ¼
                if doc_results and "Hata" in doc_results[0]:
                    st.error(f"{doc_title} iÅŸlenirken hata: {doc_results[0]['Hata']}")
                else:
                    # SonuÃ§lara Belge AdÄ±nÄ± Ekle
                    for res in doc_results:
                        res['Belge'] = doc_title
                        res['URL'] = doc_url
                        all_findings.extend(doc_results)
                
                # Ä°lerleme Ã‡ubuÄŸu
                progress_bar.progress((idx + 1) / len(selected_docs))
            
            status_text.success("Tarama TamamlandÄ±!")
            
            # --- SONUÃ‡LARI GÃ–STER ---
            if all_findings:
                st.success(f"Toplam **{len(all_findings)}** potansiyel eÅŸleÅŸme bulundu.")
                
                # DataFrame oluÅŸtur
                df_results = pd.DataFrame(all_findings)
                
                # Tabloyu dÃ¼zenle (SÃ¼tun sÄ±rasÄ±)
                df_display = df_results[['Ä°sim', 'Belge', 'Sayfa', 'BaÄŸlam', 'URL']]
                
                # Streamlit interaktif tablosu
                st.dataframe(
                    df_display,
                    column_config={
                        "URL": st.column_config.LinkColumn("Belge Linki"),
                        "BaÄŸlam": st.column_config.TextColumn("BaÄŸlam", width="large"),
                    },
                    hide_index=True,
                    use_container_width=True
                )
                
                # CSV Ä°ndirme Butonu
                csv = df_display.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="SonuÃ§larÄ± CSV Olarak Ä°ndir",
                    data=csv,
                    file_name='epstein_turkce_analiz.csv',
                    mime='text/csv',
                )
            else:
                st.info("SeÃ§ilen belgelerde veritabanÄ±ndaki TÃ¼rkÃ§e isimlere rastlanmadÄ±.")

# Footer
st.markdown("---")
st.markdown("<div style='text-align:center; color:#555;'>Bu proje aÃ§Ä±k kaynaklÄ±dÄ±r ve GitHub Ã¼zerinden Ã§alÄ±ÅŸtÄ±rÄ±labilir.</div>", unsafe_allow_html=True)


