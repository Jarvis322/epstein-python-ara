import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from pypdf import PdfReader
import io
import re
import time

# -----------------------------------------------------------------------------
# 1. AYARLAR
# -----------------------------------------------------------------------------
st.set_page_config(
    page_title="Epstein ArÅŸiv TarayÄ±cÄ±",
    page_icon="ğŸ•µï¸â€â™‚ï¸",
    layout="wide"
)

st.markdown("""
<style>
    .stApp { background-color: #0e1117; color: #c9d1d9; }
    h1, h2, h3 { color: #58a6ff; font-family: 'Segoe UI', sans-serif; }
    .stButton>button { background-color: #238636; color: white; border-radius: 6px; height: 3em; }
    .highlight { background-color: #d29922; color: #000; padding: 2px 4px; border-radius: 3px; font-weight: bold; }
    a { color: #58a6ff; text-decoration: none; }
</style>
""", unsafe_allow_html=True)

# -----------------------------------------------------------------------------
# 2. VERÄ° SETLERÄ° (HARDCODED FALLBACK)
# -----------------------------------------------------------------------------

# Site eriÅŸimi engellenirse kullanÄ±lacak "Acil Durum" listesi
FALLBACK_DOCS = [
    {"Title": "ğŸš¨ Flight Logs (UÃ§uÅŸ KayÄ±tlarÄ± - Pilot DavasÄ±)", "URL": "https://www.justice.gov/usao-sdny/case-document/file/1179426/dl"},
    {"Title": "ğŸš¨ Ana Dava DosyasÄ± (Giuffre v. Maxwell - Unsealed)", "URL": "https://www.justice.gov/usao-sdny/case-document/file/1349166/dl"},
    {"Title": "ğŸš¨ Ghislaine Maxwell Ä°fadesi (Deposition)", "URL": "https://www.justice.gov/usao-sdny/case-document/file/1349171/dl"},
    {"Title": "ğŸš¨ Epstein Savunma DosyasÄ±", "URL": "https://www.justice.gov/usao-sdny/case-document/file/1349176/dl"}
]

@st.cache_data
def get_turkish_names_dataset():
    """GeniÅŸletilmiÅŸ TÃ¼rkÃ§e Ä°sim Listesi"""
    # Buraya en yaygÄ±n 100+ isim ekledim, gerÃ§ek projede bunu JSON'dan Ã§ekersin.
    names = [
        "Ahmet", "Mehmet", "Mustafa", "AyÅŸe", "Fatma", "Hatice", "Zeynep", "Elif", "Hakan", 
        "GÃ¶kÃ§e", "Banu", "Refia", "Turabi", "Pelin", "Sultan", "Kemal", "Cem", "Can", 
        "Burak", "Emre", "Murat", "Selin", "Leyla", "Gamze", "Ece", "Neslihan", "Ozan", 
        "BarÄ±ÅŸ", "Arda", "Kerem", "Sibel", "Derya", "Deniz", "Yasemin", "Filiz", "Dilek", 
        "AslÄ±", "Melis", "Buse", "Gizem", "Merve", "Ä°rem", "Ebru", "Burcu", "Didem", "Sinem", 
        "Seda", "Esin", "Åule", "Hande", "Ali", "Veli", "Hasan", "HÃ¼seyin", "Osman", "Ã–mer",
        "Yusuf", "Ä°brahim", "Halil", "SÃ¼leyman", "Recep", "Tayyip", "Abdullah", "GÃ¼l",
        "ErdoÄŸan", "Binali", "Berat", "Bilal", "SÃ¼meyye", "Esra", "Melih", "Melih", "Melih"
    ]
    return set(names)

# -----------------------------------------------------------------------------
# 3. FONKSÄ°YONLAR
# -----------------------------------------------------------------------------

def normalize_text(text):
    if not isinstance(text, str): return ""
    translation_table = str.maketrans({
        'ÄŸ': 'g', 'Ä': 'G', 'Ã¼': 'u', 'Ãœ': 'U', 'ÅŸ': 's', 'Å': 'S',
        'Ä±': 'i', 'Ä°': 'I', 'Ã¶': 'o', 'Ã–': 'O', 'Ã§': 'c', 'Ã‡': 'C'
    })
    return text.translate(translation_table).lower()

def get_context(full_text, keyword_normalized, window=80):
    full_text_normalized = normalize_text(full_text)
    pattern = r'\b' + re.escape(keyword_normalized) + r'\b'
    matches = []
    for m in re.finditer(pattern, full_text_normalized):
        start = max(0, m.start() - window)
        end = min(len(full_text), m.end() + window)
        snippet = full_text[start:end].replace('\n', ' ').strip()
        matches.append(snippet)
    return matches

@st.cache_data(ttl=3600)
def get_documents():
    """Ã–nce siteye baÄŸlanmayÄ± dener, olmazsa yedek listeyi kullanÄ±r."""
    url = "https://www.justice.gov/epstein"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive',
    }
    
    docs = []
    status_msg = ""
    
    try:
        # Siteye baÄŸlanmayÄ± dene
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            for link in soup.find_all('a', href=True):
                href = link['href']
                if href.endswith('.pdf'):
                    full_url = href if href.startswith('http') else f"https://www.justice.gov{href}"
                    title = link.text.strip() or "Ä°simsiz Belge"
                    docs.append({"Title": title, "URL": full_url})
            status_msg = "âœ… Justice.gov sitesinden canlÄ± liste Ã§ekildi."
        else:
            raise Exception(f"HTTP {response.status_code}")
            
    except Exception as e:
        # Hata olursa yedek listeyi kullan
        status_msg = f"âš ï¸ Siteye doÄŸrudan eriÅŸilemedi ({str(e)}). Yedek liste kullanÄ±lÄ±yor."
        docs = FALLBACK_DOCS
    
    # EÄŸer site boÅŸ liste dÃ¶nerse de yedeÄŸi kullan
    if not docs:
        docs = FALLBACK_DOCS
        status_msg = "âš ï¸ Site boÅŸ yanÄ±t dÃ¶ndÃ¼. Yedek liste kullanÄ±lÄ±yor."
        
    return pd.DataFrame(docs), status_msg

def analyze_pdf(url, turkish_names_set):
    findings = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, stream=True, timeout=30)
        pdf_file = io.BytesIO(response.content)
        reader = PdfReader(pdf_file)
        
        normalized_names_set = {normalize_text(n) for n in turkish_names_set}
        
        for i, page in enumerate(reader.pages):
            text = page.extract_text()
            if not text: continue
            
            # Kelimeleri ayÄ±kla (Basit regex)
            possible_names = re.findall(r'\b[A-Za-zÄŸÃ¼ÅŸÄ±Ã¶Ã§ÄÃœÅÄ°Ã–Ã‡]+\b', text)
            page_words_normalized = {normalize_text(w) for w in possible_names}
            
            found_names_normalized = page_words_normalized.intersection(normalized_names_set)
            
            if found_names_normalized:
                for f_name in found_names_normalized:
                    # Orijinal ismi bul
                    original_name = next((n for n in turkish_names_set if normalize_text(n) == f_name), f_name)
                    contexts = get_context(text, f_name)
                    
                    for ctx in contexts:
                        findings.append({
                            "Ä°sim": original_name.upper(),
                            "Sayfa": i + 1,
                            "BaÄŸlam": f"...{ctx}..."
                        })
    except Exception as e:
        return [{"Hata": str(e)}]
    
    return findings

# -----------------------------------------------------------------------------
# 4. ARAYÃœZ
# -----------------------------------------------------------------------------

st.title("ğŸ•µï¸â€â™‚ï¸ Epstein TÃ¼rkÃ§e Ä°sim TarayÄ±cÄ±")
st.markdown("Bu araÃ§, belgeleri tarayarak veri tabanÄ±ndaki TÃ¼rkÃ§e isimlerle eÅŸleÅŸtirir.")

# 1. BELGELERÄ° GETÄ°R
with st.spinner("Belge listesi yÃ¼kleniyor..."):
    df_docs, status_message = get_documents()

if "âš ï¸" in status_message:
    st.warning(status_message)
else:
    st.success(status_message)

# 2. Ä°SÄ°M LÄ°STESÄ°
turkish_names = get_turkish_names_dataset()

# EKSTRA Ä°SÄ°M EKLEME
with st.expander("â• Aratmak istediÄŸiniz Ã¶zel isimler ekleyin"):
    custom_names = st.text_input("VirgÃ¼lle ayÄ±rarak yazÄ±n (Ã–rn: Acun, Turabi):")
    if custom_names:
        extras = {x.strip() for x in custom_names.split(',') if x.strip()}
        turkish_names.update(extras)
        st.info(f"{len(extras)} isim listeye eklendi.")

# 3. SEÃ‡Ä°M VE TARAMA
if not df_docs.empty:
    selected_docs = st.multiselect(
        "Taranacak Belgeleri SeÃ§in:", 
        df_docs['Title'].tolist(),
        default=df_docs['Title'].tolist()[:1] # Ä°lkini seÃ§ili getir
    )
    
    if st.button("ğŸš€ Analizi BaÅŸlat"):
        if not selected_docs:
            st.error("LÃ¼tfen en az bir belge seÃ§in.")
        else:
            all_findings = []
            progress = st.progress(0)
            status_box = st.empty()
            
            for idx, doc_title in enumerate(selected_docs):
                doc_data = df_docs[df_docs['Title'] == doc_title].iloc[0]
                doc_url = doc_data['URL']
                
                status_box.markdown(f"**â³ Ä°ÅŸleniyor:** `{doc_title}`")
                
                results = analyze_pdf(doc_url, turkish_names)
                
                if results and "Hata" in results[0]:
                    st.error(f"{doc_title} hatasÄ±: {results[0]['Hata']}")
                else:
                    for res in results:
                        res['Belge'] = doc_title
                        res['Link'] = doc_url
                    all_findings.extend(results)
                
                progress.progress((idx + 1) / len(selected_docs))
            
            status_box.success("Ä°ÅŸlem TamamlandÄ±!")
            
            if all_findings:
                st.balloons()
                df_results = pd.DataFrame(all_findings)
                
                st.write(f"### ğŸ¯ Toplam {len(all_findings)} EÅŸleÅŸme Bulundu")
                
                st.dataframe(
                    df_results[['Ä°sim', 'Belge', 'Sayfa', 'BaÄŸlam', 'Link']],
                    column_config={
                        "Link": st.column_config.LinkColumn("Belgeyi AÃ§"),
                        "BaÄŸlam": st.column_config.TextColumn("BaÄŸlam (Ã–nizleme)", width="large"),
                    },
                    use_container_width=True
                )
            else:
                st.info("SeÃ§ilen belgelerde veritabanÄ±ndaki isimlere rastlanmadÄ±.")
else:
    st.error("Belge listesi oluÅŸturulamadÄ±.")


